{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1EQ5G9rQcwE"
   },
   "source": [
    "# Clustering and PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Toc_OzXYQcwH"
   },
   "source": [
    "### Mushroom Dataset\n",
    "\n",
    "Podeis obtener el conjunto de datos en el siguiente enlace:\n",
    "\n",
    "[Mushroom Dataset](https://www.kaggle.com/uciml/mushroom-classification)\n",
    "\n",
    "Como podr√©is comprobar, hay muchas variables, todas ellas categ√≥ricas, por lo que exploraciones con scatterplot no nos ser√°n √∫tiles como en otros casos.\n",
    "\n",
    "La variable a predecir ``class`` es binaria.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJc845c11KbK"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RWIOS2y_QcwH"
   },
   "outputs": [],
   "source": [
    "# Carga de librer√≠as, las que hemos considerado b√°sicas, a√±adid lo que quer√°is :)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KbJPDrpQcwI"
   },
   "source": [
    "### Leer conjunto de datos y primer vistazo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zFJoAIsVQcwI"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# URL del dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data\"\n",
    "\n",
    "# Nombres de las columnas seg√∫n la documentaci√≥n oficial\n",
    "column_names = [\n",
    "    \"class\", \"cap-shape\", \"cap-surface\", \"cap-color\", \"bruises\", \"odor\",\n",
    "    \"gill-attachment\", \"gill-spacing\", \"gill-size\", \"gill-color\", \"stalk-shape\",\n",
    "    \"stalk-root\", \"stalk-surface-above-ring\", \"stalk-surface-below-ring\",\n",
    "    \"stalk-color-above-ring\", \"stalk-color-below-ring\", \"veil-type\", \"veil-color\",\n",
    "    \"ring-number\", \"ring-type\", \"spore-print-color\", \"population\", \"habitat\"\n",
    "]\n",
    "\n",
    "# Leer el dataset desde la URL, sin cabeceras, usando los nombres definidos\n",
    "df = pd.read_csv(url, header=None, names=column_names)\n",
    "\n",
    "# Mostrar las primeras 5 filas del dataframe\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga del dataset\n",
    "\n",
    "Comenzamos leyendo el dataset de setas directamente desde la fuente oficial de la UCI.  \n",
    "Este archivo no incluye nombres de columna, por lo que se los a√±adimos manualmente seg√∫n la documentaci√≥n del dataset.\n",
    "\n",
    "Cada fila representa un hongo, y cada columna describe una caracter√≠stica (forma, color, olor, etc.).\n",
    "\n",
    "La columna `class` indica si el hongo es comestible (`e`) o venenoso (`p`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dePM9qXKQcwJ"
   },
   "source": [
    "### Exploraci√≥n de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45TsUuwkQcwJ"
   },
   "outputs": [],
   "source": [
    "# Mostrar informaci√≥n general del DataFrame\n",
    "df.info()\n",
    "\n",
    "# Mostrar n√∫mero de filas, columnas y valores √∫nicos por variable\n",
    "print(f\"\\nN√∫mero de filas: {df.shape[0]}\")\n",
    "print(f\"N√∫mero de columnas: {df.shape[1]}\")\n",
    "print(\"\\nValores √∫nicos por variable:\")\n",
    "print(df.nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripci√≥n del conjunto de datos\n",
    "\n",
    "En este bloque obtenemos una visi√≥n general del dataset:\n",
    "\n",
    "- `df.info()` nos muestra cu√°ntos valores hay por columna y el tipo de datos.\n",
    "- `df.shape` nos indica el n√∫mero de instancias (filas) y variables (columnas).\n",
    "- `df.nunique()` nos dice cu√°ntos valores diferentes hay en cada variable, lo cual nos ayuda a entender la variedad en los datos.\n",
    "\n",
    "Dado que todas las variables son categ√≥ricas, es normal que el tipo de dato sea `object`.\n",
    "Este an√°lisis nos ayuda a detectar posibles columnas poco informativas (con un solo valor) o con demasiados valores √∫nicos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Md7i8gXBQcwJ"
   },
   "source": [
    "#### Calcular el n√∫mero de nulos de cada feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8xXz4mT0QcwJ"
   },
   "outputs": [],
   "source": [
    "# Contar valores nulos est√°ndar (NaN)\n",
    "print(\"Valores nulos por columna:\\n\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Contar valores que en realidad son '?' (suelen aparecer como texto en datasets antiguos)\n",
    "print(\"\\nValores '?' por columna (posibles nulos codificados):\\n\")\n",
    "print((df == '?').sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B√∫squeda de valores nulos\n",
    "\n",
    "Primero buscamos valores nulos reales (`NaN`) con `df.isnull().sum()`.\n",
    "\n",
    "Sin embargo, en muchos datasets antiguos como este, los valores faltantes est√°n representados como s√≠mbolos como `'?'`.\n",
    "\n",
    "Por eso tambi√©n contamos cu√°ntos `'?'` hay por columna. Esto nos permitir√° decidir si imputar, eliminar o transformar esos datos en el preprocesamiento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJv-ez3WQcwK"
   },
   "source": [
    "#### Buscar valores extra√±os. Para ello, ver los valores √∫nicos en cada feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fUZ2EHmTQcwK"
   },
   "outputs": [],
   "source": [
    "# Crear un nuevo DataFrame con el n√∫mero de valores √∫nicos por columna\n",
    "features_summary = pd.DataFrame({\n",
    "    \"features\": df.columns,\n",
    "    \"n_values\": df.nunique().values\n",
    "})\n",
    "\n",
    "# Mostrar el resumen\n",
    "features_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valores √∫nicos por variable\n",
    "\n",
    "Este resumen nos permite identificar de forma clara cu√°ntos valores diferentes tiene cada variable.\n",
    "\n",
    "Esto nos ayuda a detectar:\n",
    "\n",
    "- Variables con un solo valor (poco informativas, probablemente eliminables)\n",
    "- Variables con muchos valores distintos (pueden requerir codificaci√≥n especial)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXIyz_tdQcwK"
   },
   "source": [
    "#### Tratar aquellos valores que entendamos que sean nulos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OVQnxK1gQcwK"
   },
   "outputs": [],
   "source": [
    "# Reemplazar los '?' de la columna 'stalk-root' con la moda\n",
    "modo = df['stalk-root'].mode()[0]\n",
    "df['stalk-root'] = df['stalk-root'].replace('?', modo)\n",
    "\n",
    "# Verificar que ya no hay '?'\n",
    "(df == '?').sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputaci√≥n de valores faltantes\n",
    "\n",
    "La columna `stalk-root` contiene valores faltantes representados como `'?'`.\n",
    "\n",
    "Hemos decidido **imputarlos con la moda** (el valor m√°s frecuente de la columna), ya que esto:\n",
    "\n",
    "- Evita perder datos\n",
    "- Evita tratar `'?'` como si fuera una categor√≠a real\n",
    "- Mantiene la consistencia para t√©cnicas como PCA y KMeans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dbmx1Z7QcwK"
   },
   "source": [
    "#### Mirad cu√°ntos valores hay en cada feature, ¬øTodas las features aportan informaci√≥n? Si alguna no aporta informaci√≥n, eliminadla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ts2xeUavQcwK"
   },
   "outputs": [],
   "source": [
    "# Identificar columnas con un solo valor √∫nico\n",
    "columnas_constantes = features_summary[features_summary[\"n_values\"] == 1][\"features\"].tolist()\n",
    "print(\"Columnas que se eliminar√°n por tener un solo valor:\")\n",
    "print(columnas_constantes)\n",
    "\n",
    "# Eliminar del DataFrame original\n",
    "df.drop(columns=columnas_constantes, inplace=True)\n",
    "\n",
    "# Verificar dimensiones\n",
    "print(f\"\\nNueva forma del DataFrame: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminaci√≥n de variables poco informativas\n",
    "\n",
    "Eliminamos las columnas que tienen **un solo valor √∫nico**, ya que no aportan variabilidad al modelo.\n",
    "\n",
    "Estas columnas no ayudan a diferenciar entre clases ni influyen en los componentes principales o en la agrupaci√≥n de datos.\n",
    "\n",
    "En nuestro caso, la columna `veil-type` tiene un √∫nico valor y por tanto se elimina.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dX1LM1VQcwK"
   },
   "source": [
    "#### Separar entre variables predictoras y variables a predecir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "pS9HEA2eQcwK",
    "outputId": "3bc8f166-5b22-41e7-aa6b-69bad0b06280"
   },
   "outputs": [],
   "source": [
    "# Variable objetivo\n",
    "y = df[\"class\"]\n",
    "\n",
    "# Variables predictoras\n",
    "X = df.drop(columns=[\"class\"])\n",
    "\n",
    "# Comprobamos formas\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separaci√≥n de variables predictoras y objetivo\n",
    "\n",
    "Separamos la variable que queremos predecir (`class`) del resto de variables.\n",
    "\n",
    "- `y` contiene la columna `class`, que indica si un hongo es comestible (`e`) o venenoso (`p`).\n",
    "- `X` contiene todas las dem√°s columnas, que usaremos como variables explicativas.\n",
    "\n",
    "Este paso es fundamental para entrenar modelos supervisados o para evaluar c√≥mo se estructuran los datos sin usar la etiqueta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sN1fZZfZQcwL"
   },
   "source": [
    "#### Codificar correctamente las variables categ√≥ricas a num√©ricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-4l92dfEQcwL"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Inicializamos el codificador\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Aplicamos OneHotEncoder a X\n",
    "X_encoded = encoder.fit_transform(X)\n",
    "\n",
    "# Convertimos a DataFrame para mayor claridad\n",
    "X_encoded_df = pd.DataFrame(X_encoded, columns=encoder.get_feature_names_out(X.columns))\n",
    "\n",
    "# Mostramos las primeras filas del nuevo DataFrame codificado\n",
    "X_encoded_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codificaci√≥n de variables categ√≥ricas\n",
    "\n",
    "Usamos `OneHotEncoder` para transformar todas las variables categ√≥ricas en variables num√©ricas.\n",
    "\n",
    "Este proceso convierte cada categor√≠a en una nueva columna binaria (0 o 1), eliminando ambig√ºedad y permitiendo que algoritmos como PCA y KMeans trabajen correctamente.\n",
    "\n",
    "Por ejemplo:  \n",
    "Una columna con tres valores posibles (`rojo`, `verde`, `azul`) se convierte en tres columnas:\n",
    "- `color_rojo`, `color_verde`, `color_azul`\n",
    "\n",
    "Este paso aumenta la dimensionalidad, pero es necesario para aplicar t√©cnicas matem√°ticas a datos categ√≥ricos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RBBAM0bzQcwL"
   },
   "source": [
    "#### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hHxqeJQqQcwL"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Divisi√≥n del dataset (usamos X e y originales antes de codificar si el modelo requiere categor√≠as)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Comprobamos formas\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(f\"y_test: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divisi√≥n en entrenamiento y test\n",
    "\n",
    "Dividimos el conjunto de datos en:\n",
    "\n",
    "- `X_train` / `y_train`: para entrenar modelos\n",
    "- `X_test` / `y_test`: para evaluar su rendimiento\n",
    "\n",
    "Reservamos un **33% para test**, usando `random_state=42` para asegurar que todos obtengamos la misma divisi√≥n.\n",
    "\n",
    "Esto es fundamental cuando comparamos modelos supervisados como Random Forest m√°s adelante.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¬øPor qu√© hacemos un Train/Test Split?\n",
    "\n",
    "En este paso separamos nuestros datos en dos subconjuntos:\n",
    "\n",
    "- `X_train`, `y_train`: se usan para entrenar los modelos.\n",
    "- `X_test`, `y_test`: se usan para evaluar c√≥mo se comporta el modelo con datos que **no ha visto** antes.\n",
    "\n",
    "Esto nos permite medir la capacidad del modelo para generalizar a nuevos datos.\n",
    "\n",
    "---\n",
    "\n",
    "### ¬øQu√© estamos dividiendo?\n",
    "\n",
    "- `X`: todas las variables predictoras (caracter√≠sticas del hongo)\n",
    "- `y`: la clase que indica si el hongo es comestible (`e`) o venenoso (`p`)\n",
    "\n",
    "---\n",
    "\n",
    "### ¬øPor qu√© es importante?\n",
    "\n",
    "Esta divisi√≥n simula un escenario del mundo real:\n",
    "> Entrenas tu modelo con datos hist√≥ricos y luego lo usas para hacer predicciones sobre nuevos datos que van llegando.\n",
    "\n",
    "Adem√°s, **evita el sobreajuste (overfitting)**, es decir, que el modelo se aprenda de memoria los datos del entrenamiento y no sepa generalizar.\n",
    "\n",
    "---\n",
    "\n",
    "### ¬øPor qu√© `random_state=42`?\n",
    "\n",
    "El par√°metro `random_state` fija la semilla aleatoria para que la partici√≥n sea siempre la misma al ejecutar el c√≥digo.  \n",
    "Esto permite reproducibilidad: t√∫ y tus compa√±eros obtendr√©is el mismo resultado.\n",
    "\n",
    "---\n",
    "\n",
    "### Nota\n",
    "\n",
    "Aunque este paso es fundamental para modelos **supervisados** (como Random Forest),  \n",
    "tambi√©n nos permite **comparar resultados con modelos no supervisados**, como clustering, en una fase posterior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3sL57bzCQcwL"
   },
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhAhJqbKQcwM"
   },
   "source": [
    "Es un conjunto de datos del que a√∫n no hemos visto nada (no tenemos graficas) as√≠ que vamos a hacer algunas. Tenemos el problema de que son muchas variables, **PCA al rescate**: le pedimos que nos de dos dimensiones y las pintamos, sabemos que ser√°n **aquellas que retengan m√°s informaci√≥n**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3232KSn9QcwM"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Codificamos X_train con OneHotEncoder para aplicar PCA\n",
    "X_train_encoded = encoder.fit_transform(X_train)\n",
    "\n",
    "# Aplicamos PCA para reducir a 2 dimensiones\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train_encoded)\n",
    "\n",
    "# Creamos un DataFrame para graficar m√°s c√≥modamente\n",
    "df_pca = pd.DataFrame(data=X_train_pca, columns=[\"PC1\", \"PC2\"])\n",
    "df_pca[\"class\"] = y_train.values\n",
    "\n",
    "# Visualizaci√≥n en scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    x=\"PC1\", y=\"PC2\", hue=\"class\", palette={\"e\": \"green\", \"p\": \"red\"}, data=df_pca, alpha=0.7\n",
    ")\n",
    "plt.title(\"Visualizaci√≥n del conjunto de entrenamiento con PCA (2 componentes)\")\n",
    "plt.xlabel(\"Primer componente principal (PC1)\")\n",
    "plt.ylabel(\"Segundo componente principal (PC2)\")\n",
    "plt.legend(title=\"Clase\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducci√≥n de dimensionalidad con PCA\n",
    "\n",
    "El dataset tiene muchas variables categ√≥ricas que, tras ser codificadas con One-Hot Encoding, generan **una matriz de alta dimensi√≥n**.\n",
    "\n",
    "Para poder **visualizar los datos en un plano (2D)**, aplicamos PCA (An√°lisis de Componentes Principales), una t√©cnica que transforma los datos y reduce su dimensionalidad reteniendo la **mayor varianza posible**.\n",
    "\n",
    "En este scatter plot representamos los datos proyectados sobre los **dos primeros componentes principales**, coloreando los puntos seg√∫n la clase (`comestible` o `venenoso`).\n",
    "\n",
    "Este tipo de visualizaci√≥n nos permite intuir si hay separaci√≥n o estructura natural entre las clases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMYH_Hv0QcwM"
   },
   "source": [
    "Parece que est√° bastante separadito, parece que a ojo mucho se puede ver :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QdE0AvlKQcwM"
   },
   "source": [
    "Igualmente, vamos a entrenar un clasificador a ver qu√© tal lo hace antes de editar m√°s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eKQqz_EPQcwM"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Codificamos X_train y X_test con OneHotEncoder\n",
    "X_train_encoded = encoder.fit_transform(X_train)\n",
    "X_test_encoded = encoder.transform(X_test)\n",
    "\n",
    "# Entrenamos el modelo\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred = clf.predict(X_test_encoded)\n",
    "\n",
    "# Evaluaci√≥n del modelo\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nMatriz de confusi√≥n:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nReporte de clasificaci√≥n:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificador supervisado: Random Forest\n",
    "\n",
    "Entrenamos un modelo supervisado con Random Forest para tener un **punto de referencia**.\n",
    "\n",
    "Este modelo intenta aprender la relaci√≥n entre las caracter√≠sticas de los hongos (`X_train`) y su clase (`y_train`).\n",
    "\n",
    "Despu√©s evaluamos su rendimiento con los datos de test (`X_test`, `y_test`), midiendo:\n",
    "\n",
    "- Precisi√≥n (`accuracy`)\n",
    "- Matriz de confusi√≥n\n",
    "- M√©tricas detalladas por clase (`precision`, `recall`, `f1-score`)\n",
    "\n",
    "Este resultado nos sirve como referencia para comparar con m√©todos no supervisados como KMeans.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PACQlU5_QcwM"
   },
   "source": [
    "Es un conjunto sencillo y Random Forest es muy bueno en su trabajo, Igualmente, vamos a ver qu√© tama√±o tenemos de dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ODibK0D2QcwN"
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tama√±o del conjunto de entrenamiento\n",
    "\n",
    "El conjunto `X_train` contiene **5443 muestras** y **21 variables predictoras**.\n",
    "\n",
    "Esto significa que:\n",
    "\n",
    "- Estamos entrenando nuestros modelos con 5443 hongos distintos, cada uno descrito por 21 caracter√≠sticas.\n",
    "- Estas caracter√≠sticas son variables categ√≥ricas como forma, color, superficie del sombrero, olor, etc.\n",
    "- La columna `class` no se incluye aqu√≠ porque ya la hemos separado como variable objetivo (`y_train`).\n",
    "\n",
    "Es importante conocer esta forma para:\n",
    "\n",
    "- Evaluar la complejidad del modelo (n√∫mero de muestras vs n√∫mero de variables)\n",
    "- Preparar correctamente el preprocesamiento (por ejemplo, One-Hot Encoding aumentar√° el n√∫mero de columnas significativamente)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rEVhvRaQcwN"
   },
   "source": [
    "¬øMuchas features no? Vamos a reducir las usando PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kEJPZw_cQcwN"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Re-codificamos todo el conjunto para trabajar con variables num√©ricas\n",
    "X_train_encoded = encoder.fit_transform(X_train)\n",
    "X_test_encoded = encoder.transform(X_test)\n",
    "\n",
    "# Probamos diferentes n√∫meros de componentes principales\n",
    "n_features = list(range(2, X_train_encoded.shape[1] + 1, 5))  # Ej: [2, 7, 12, ..., 92]\n",
    "scores = []\n",
    "\n",
    "for n in n_features:\n",
    "    # 1. Reducir dimensionalidad con PCA\n",
    "    pca = PCA(n_components=n)\n",
    "    X_train_pca = pca.fit_transform(X_train_encoded)\n",
    "    X_test_pca = pca.transform(X_test_encoded)\n",
    "\n",
    "    # 2. Entrenar clasificador con los datos reducidos\n",
    "    clf = RandomForestClassifier(random_state=42)\n",
    "    clf.fit(X_train_pca, y_train)\n",
    "    y_pred = clf.predict(X_test_pca)\n",
    "\n",
    "    # 3. Guardar la precisi√≥n\n",
    "    score = accuracy_score(y_test, y_pred)\n",
    "    scores.append(score)\n",
    "\n",
    "# Visualizar el rendimiento seg√∫n el n√∫mero de componentes\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(x=n_features, y=scores, marker=\"o\")\n",
    "plt.title(\"Precisi√≥n del clasificador vs. n√∫mero de componentes PCA\")\n",
    "plt.xlabel(\"N√∫mero de componentes\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducci√≥n de dimensionalidad con PCA y evaluaci√≥n supervisada\n",
    "\n",
    "Queremos saber cu√°ntas componentes principales son suficientes para mantener un buen rendimiento del clasificador.\n",
    "\n",
    "Para ello:\n",
    "\n",
    "1. Aplicamos PCA sobre los datos con un n√∫mero creciente de componentes (`n_features`).\n",
    "2. Entrenamos un modelo de Random Forest sobre esos datos reducidos.\n",
    "3. Medimos su precisi√≥n sobre el conjunto de test.\n",
    "\n",
    "Este gr√°fico nos permite identificar un punto de equilibrio:  \n",
    "el menor n√∫mero de componentes que mantiene un rendimiento aceptable.\n",
    "\n",
    "As√≠ reducimos la complejidad del modelo sin sacrificar precisi√≥n.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An√°lisis de resultados: reducci√≥n dr√°stica sin p√©rdida de precisi√≥n\n",
    "\n",
    "Tras aplicar PCA sobre los datos codificados, observamos que con solo **10 componentes principales** el modelo Random Forest alcanza pr√°cticamente la **misma precisi√≥n** que con todas las variables.\n",
    "\n",
    "Esto significa que:\n",
    "\n",
    "- Hemos pasado de ~95 columnas (tras OneHotEncoding) a solo 10.\n",
    "- Estamos utilizando aproximadamente un **10% de las variables transformadas**.\n",
    "- Incluso hemos reducido m√°s de la mitad respecto a las **21 variables originales** del dataset sin codificar.\n",
    "\n",
    "Este resultado demuestra que:\n",
    "\n",
    "‚úîÔ∏è Muchas de las variables codificadas eran **redundantes o poco informativas**.  \n",
    "‚úîÔ∏è PCA ha sido muy eficaz para condensar la informaci√≥n en menos dimensiones.  \n",
    "‚úîÔ∏è La reducci√≥n de dimensionalidad no solo mejora la eficiencia, sino que **mantiene (o incluso mejora) la capacidad de predicci√≥n** del modelo.\n",
    "\n",
    "Este es un gran ejemplo de c√≥mo el preprocesamiento inteligente mejora los modelos sin necesidad de complejidad extra.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Jvqa-leQcwN"
   },
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWiMHKUdQcwN"
   },
   "source": [
    "Viendo que el conjunto de datos es sencillito, podemos intentar hacer algo de clustering a ver qu√© informaci√≥n podemos obtener.\n",
    "\n",
    "El primer paso va a ser importar la funci√≥n de Kmeans de sklearn, y a partir de ahi, vamos a buscar el valor √≥ptimo de clusters. Como hemos visto anteriormente, este valor lo obtenemos, por ejemplo, del codo de la gr√°fica que representa el total de las distancias de los puntos a los centros de los clusters asociados. Os dejo la p√°gina de la documentaci√≥n de sklearn para que lo busqu√©is:\n",
    "\n",
    "[K-Means on sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)\n",
    "\n",
    "Con esto solo hay que ahora generar los modelos de kmeans, evaluar y pintar la gr√°fica para los valores de ``k`` que establezcais.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DV0IXFncQcwO"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Aplicamos PCA para reducir a 2 dimensiones antes de hacer clustering\n",
    "# (si no lo has hecho ya en una celda anterior)\n",
    "X_encoded = encoder.fit_transform(X)\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_encoded)\n",
    "\n",
    "# Rango de valores de k (n√∫mero de clusters a probar)\n",
    "k_values = range(1, 11)\n",
    "scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    # Definir modelo KMeans\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
    "    kmeans.fit(X_pca)\n",
    "    \n",
    "    # Guardamos la suma de distancias dentro del cluster (inertia)\n",
    "    scores.append(kmeans.inertia_)\n",
    "\n",
    "# Representaci√≥n del m√©todo del codo\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.lineplot(x=list(k_values), y=scores, marker=\"o\")\n",
    "plt.title(\"M√©todo del codo - Selecci√≥n del n√∫mero √≥ptimo de clusters\")\n",
    "plt.xlabel(\"N√∫mero de clusters (k)\")\n",
    "plt.ylabel(\"Inercia (suma de distancias a los centroides)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering no supervisado con KMeans\n",
    "\n",
    "Ahora probamos un enfoque **no supervisado**, aplicando `KMeans` sobre los datos reducidos con PCA (2 dimensiones).\n",
    "\n",
    "El objetivo es descubrir si existen agrupaciones naturales en los datos sin usar la clase (`e` o `p`).\n",
    "\n",
    "Para seleccionar el n√∫mero √≥ptimo de clusters (`k`), usamos el **m√©todo del codo**:\n",
    "- Calculamos la **inercia** (suma de distancias de los puntos a sus centros de cluster) para diferentes valores de `k`.\n",
    "- La gr√°fica resultante nos permite identificar un ‚Äúcodo‚Äù donde el beneficio de a√±adir m√°s clusters deja de ser significativo.\n",
    "\n",
    "Este punto suele indicar el n√∫mero ideal de clusters a usar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSgPG286QcwO"
   },
   "source": [
    "Con el valor que hay√°is obtenido de la gr√°fica, pod√©is obtener una buena aproximaci√≥n de Kmeans y con ello podemos pasar a explorar c√≥mo de bien han separado la informaci√≥n los distintos clusters. Para ello, se va a hacer un ``catplot``, seaborn os lo har√° solito. Con esto lo que se pretende ver es la distribuci√≥n de la varaible a predecir en funci√≥n del cluster que haya determinado Kmeans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wa7XfETyQcwO"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Asegurarse de tener los datos codificados y reducidos\n",
    "X_encoded = encoder.fit_transform(X)\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_encoded)\n",
    "\n",
    "# Definimos y entrenamos KMeans con k=2\n",
    "kmeans = KMeans(n_clusters=2, random_state=42, n_init='auto')\n",
    "clusters = kmeans.fit_predict(X_pca)\n",
    "\n",
    "# Creamos un nuevo DataFrame con la clase real y el cluster asignado\n",
    "df_clusters = pd.DataFrame({\n",
    "    \"class\": y.values,\n",
    "    \"cluster\": clusters\n",
    "})\n",
    "\n",
    "# Pintamos con catplot: distribuci√≥n de clases dentro de cada cluster\n",
    "ax = sns.catplot(\n",
    "    col=\"cluster\",\n",
    "    x=\"class\",\n",
    "    data=df_clusters,\n",
    "    kind=\"count\",\n",
    "    col_wrap=2,\n",
    "    palette={\"e\": \"green\", \"p\": \"red\"}\n",
    ")\n",
    "ax.fig.suptitle(\"Distribuci√≥n de clases reales por cluster (KMeans)\", y=1.05)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluaci√≥n visual de los clusters\n",
    "\n",
    "Aunque KMeans no usa la variable `class`, podemos comparar sus resultados con las etiquetas reales para ver si hay alguna relaci√≥n.\n",
    "\n",
    "Usamos `sns.catplot` para representar la distribuci√≥n de clases (`e` o `p`) dentro de cada cluster creado por KMeans.\n",
    "\n",
    "Esto nos permite evaluar si el algoritmo de clustering ha conseguido **separar razonablemente bien los hongos comestibles y venenosos** sin necesidad de entrenamiento supervisado.\n",
    "\n",
    "Si un cluster contiene mayoritariamente una sola clase, podemos considerar que la separaci√≥n es significativa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JzMUKFwzQcwO"
   },
   "source": [
    "Vamos a ver qu√© tal queda esto pintado. Para ello, repetimos el scatterplot de antes pero usando como color el cluster asignado por kmeans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IjhjuexcQcwO"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Aseg√∫rate de que X_encoded est√° disponible\n",
    "X_encoded = encoder.fit_transform(X)\n",
    "\n",
    "# Reducimos a 2 componentes con PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_encoded)\n",
    "\n",
    "# Entrenamos KMeans (si no lo has hecho ya)\n",
    "kmeans = KMeans(n_clusters=2, random_state=42, n_init='auto')\n",
    "clusters = kmeans.fit_predict(X_pca)\n",
    "\n",
    "# Creamos un DataFrame para graficar\n",
    "df_pca_clusters = pd.DataFrame(X_pca, columns=[\"PC1\", \"PC2\"])\n",
    "df_pca_clusters[\"cluster\"] = clusters\n",
    "\n",
    "# Visualizamos los clusters en el espacio PCA\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    x=\"PC1\", y=\"PC2\", hue=\"cluster\", palette=\"Set1\", data=df_pca_clusters, alpha=0.7\n",
    ")\n",
    "plt.title(\"Clusters obtenidos por KMeans en el espacio PCA\")\n",
    "plt.xlabel(\"Componente principal 1\")\n",
    "plt.ylabel(\"Componente principal 2\")\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizaci√≥n de clusters en espacio PCA\n",
    "\n",
    "En este scatterplot proyectamos los datos en 2D usando PCA y coloreamos cada punto seg√∫n el **cluster asignado por KMeans**.\n",
    "\n",
    "Esto nos permite ver gr√°ficamente **c√≥mo ha agrupado los datos el algoritmo de clustering**, sin necesidad de etiquetas.\n",
    "\n",
    "Si los clusters aparecen claramente separados, significa que **KMeans ha identificado patrones naturales en los datos**, incluso sin saber qu√© clase corresponde a cada punto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0q-ZDhQQcwO"
   },
   "source": [
    "¬øEs bastante parecido no? No es tan bueno como el Random Forest, pero ha conseguido identificar bastante bien los distintos puntos del dataset sin utilizar las etiquetas. De hecho, el diagrama de factor que hemos visto antes muestra que solo un par de clusters son imprecisos. Si no hubieramos tenido etiquetas esta aproximacion nos hubiera ayudado mucho a clasificar los distintos tipos de hongos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Conclusiones finales\n",
    "\n",
    "Al comparar los resultados de KMeans (no supervisado) con los de Random Forest (supervisado), observamos lo siguiente:\n",
    "\n",
    "- El **Random Forest** obtuvo una clasificaci√≥n excelente, como era de esperar, ya que utiliza las etiquetas durante el entrenamiento.\n",
    "- El algoritmo **KMeans**, aunque no tiene acceso a las etiquetas (`class`), ha conseguido **agrupar correctamente la mayor√≠a de los puntos** del dataset en dos clusters bien diferenciados.\n",
    "- El gr√°fico de `catplot` mostr√≥ que **solo algunos clusters tienen mezcla de clases**, pero en general la separaci√≥n es bastante buena.\n",
    "- La visualizaci√≥n en el espacio PCA con colores por cluster confirma que los **datos tienen una estructura latente clara**, que puede ser aprovechada incluso sin supervisi√≥n.\n",
    "\n",
    "üìå **Si no hubi√©ramos tenido etiquetas**, esta aproximaci√≥n basada en PCA + KMeans nos habr√≠a servido para **detectar dos grupos principales** en el dataset de hongos, lo cual ya ofrece un valor muy significativo en tareas de exploraci√≥n y descubrimiento de patrones.\n",
    "\n",
    "En resumen:\n",
    "\n",
    "> Aunque el modelo no supervisado no alcanza la precisi√≥n del supervisado, ha demostrado ser una **herramienta √∫til y potente** para clasificar y entender datos complejos sin necesidad de etiquetas.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
